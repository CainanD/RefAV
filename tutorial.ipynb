{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RefAV Tutorial\n",
    "To start, we separate the scenario mining ground truth annotations into separate log folders.\n",
    "\n",
    "At the same time, we create the ground truth .pkl files we can use to evaluate performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from refAV.paths import AV2_DATA_DIR, SM_DATA_DIR\n",
    "from refAV.dataset_conversion import separate_scenario_mining_annotations, create_gt_mining_pkls_parallel\n",
    "\n",
    "sm_val_feather = Path('av2_sm_downloads/scenario_mining_val_annotations.feather')\n",
    "separate_scenario_mining_annotations(sm_val_feather, SM_DATA_DIR)\n",
    "create_gt_mining_pkls_parallel(sm_val_feather, SM_DATA_DIR, num_processes=max(1, int(.9*os.cpu_count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RefAV takes any base set of predicted tracks runs a set of filtering operations to identify the relevant portions of each track.\n",
    "The baseline uses the track predictions from the winner of the Argoverse 2 End-to-End Tracking and Forecasting challenge, Le3DE2E. This block downloads the track predictions for the val set from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from refAV.dataset_conversion import pickle_to_feather\n",
    "from refAV.paths import SM_PRED_DIR\n",
    "\n",
    "repo_id = \"CainanD/AV2_Tracker_Predictions\"\n",
    "filename = \"Le3DE2E_tracking_predictions_val.pkl\"\n",
    "tracker_predictions_dir = 'tracker_predictions'\n",
    "\n",
    "hf_hub_download(repo_id, filename, repo_type='dataset', local_dir=tracker_predictions_dir)\n",
    "\n",
    "tracking_val_predictions = Path(tracker_predictions_dir + '/' + filename)\n",
    "sm_val_feather = Path('av2_sm_downloads/scenario_mining_val_annotations.feather')\n",
    "\n",
    "pickle_to_feather(AV2_DATA_DIR, tracking_val_predictions, SM_PRED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RefAV works by constructing compositional functions that can be used to define a scenario.\n",
    "\n",
    "Here is an example of using the compositional functions to define a scenario corresponding \n",
    "to a \"moving vehicle behind another vehicle being crossed by a jaywalking pedestrian'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refAV.utils import *\n",
    "from refAV.paths import SM_PRED_DIR\n",
    "\n",
    "dataset_dir = SM_DATA_DIR / 'val'\n",
    "output_dir = SM_PRED_DIR / 'val'\n",
    "log_id = '0b86f508-5df9-4a46-bc59-5b9536dbde9f'\n",
    "log_dir = dataset_dir / log_id\n",
    "\n",
    "description = 'moving vehicle behind another vehicle being crossed by a jaywalking pedestrian'\n",
    "\n",
    "peds = get_objects_of_category(log_dir, category='PEDESTRIAN')\n",
    "peds_on_road = on_road(peds, log_dir)\n",
    "jaywalking_peds = scenario_not(at_pedestrian_crossing)(peds_on_road, log_dir)\n",
    "\n",
    "vehicles = get_objects_of_category(log_dir, category='VEHICLE')\n",
    "moving_vehicles = scenario_and([in_drivable_area(vehicles, log_dir), scenario_not(stationary)(vehicles, log_dir)])\n",
    "crossed_vehicles = being_crossed_by(moving_vehicles, jaywalking_peds, log_dir)\n",
    "behind_crossed_vehicle = get_objects_in_relative_direction(crossed_vehicles, moving_vehicles, log_dir,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdirection='backward', max_number=1, within_distance=25)\n",
    "\n",
    "#Output scenario outputs a .pkl for the predicted tracks during that scenario\n",
    "output_scenario(behind_crossed_vehicle, description, log_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to define a scenario, let's let an LLM do it for us.\n",
    "\n",
    "Using this function requires an [Anthropic API](https://www.anthropic.com/api) key!\n",
    "\n",
    "Since this can get quite expensive,\n",
    "we provide the predicted scenario definitions in the output/llm_scenario_predictions folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refAV.scenario_prediction import predict_scenario_from_description\n",
    "from refAV.paths import LLM_DEF_DIR\n",
    "\n",
    "predict_scenario_from_description('vehicle heading toward ego from the side while at an intersection', LLM_DEF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basics out of the way, let's run evaluation on the entire validation dataset.\n",
    "The create_base_prediction function calls the LLM scenario definition generator and the\n",
    " runs the defintion to find instance of the prompt.\n",
    "  It can take quite a bit of time to go through all of the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from refAV.eval import create_baseline_prediction\n",
    "\n",
    "log_prompt_input_path = Path('av2_sm_downloads/log_prompt_pairs_val.json')\n",
    "with open(log_prompt_input_path, 'rb') as f:\n",
    "\tlog_prompts = json.load(f)\n",
    "\n",
    "for log_id, prompts in tqdm(log_prompts.items()):\n",
    "\tfor prompt in prompts:\n",
    "\t\tcreate_baseline_prediction(prompt, log_id, SM_PRED_DIR, LLM_DEF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combine_matching_pkls function will combine all prediction and ground truth .pkl files into a single .pkl file. This is the .pkl file that is used for submission to the leaderboard. Running evaluate_pkls will the predicted tracks across four metrics: HOTA-Temporal, HOTA, timestamp-level F1, and scenario-level F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refAV.eval import combine_matching_pkls, evaluate_pkls\n",
    "\n",
    "eval_output_dir = Path(f'output/evaluation/val')\n",
    "combine_matching_pkls(SM_DATA_DIR, SM_PRED_DIR, eval_output_dir)\n",
    "evaluate_pkls(eval_output_dir / 'combined_predictions.pkl', eval_output_dir / 'combined_gt.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the evaluate function executes successfully, your predictions .pkl file is ready to submit to the EvalAI server for evaluation. Create a profile at EvalAI [EvalAI](https://eval.ai/) in to receive an account token. This code will submit to the validation set.\n",
    "\n",
    "```bash\n",
    "pip install evalai\n",
    "evalai set_token <EvalAI_account_token>\n",
    "evalai challenge 2469 phase 4899 submit --file output/evaluation/val/combined_predictions.pkl --large\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
