{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RefAV Tutorial\n",
    "To start, we separate the scenario mining ground truth annotations into separate log folders.\n",
    "\n",
    "At the same time, we create the ground truth .pkl files we can use to evaluate performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from refAV.paths import AV2_DATA_DIR, SM_DATA_DIR\n",
    "from refAV.dataset_conversion import separate_scenario_mining_annotations, create_gt_mining_pkls_parallel\n",
    "\n",
    "sm_val_feather = Path('av2_sm_downloads/scenario_mining_val_annotations.feather')\n",
    "separate_scenario_mining_annotations(sm_val_feather, SM_DATA_DIR)\n",
    "create_gt_mining_pkls_parallel(sm_val_feather, SM_DATA_DIR, num_processes=max(1, int(.5*os.cpu_count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RefAV works by constructing compositional functions that can be used to define a scenario.\n",
    "\n",
    "Here is an example of using the compositional functions to define a scenario corresponding \n",
    "to a \"moving vehicle behind another vehicle being crossed by a jaywalking pedestrian'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refAV.utils import *\n",
    "from refAV.paths import SM_PRED_DIR\n",
    "\n",
    "dataset_dir = SM_DATA_DIR / 'val'\n",
    "output_dir = SM_PRED_DIR / 'val'\n",
    "log_id = '0b86f508-5df9-4a46-bc59-5b9536dbde9f'\n",
    "log_dir = dataset_dir / log_id\n",
    "\n",
    "description = 'moving vehicle behind another vehicle being crossed by a jaywalking pedestrian'\n",
    "\n",
    "peds = get_objects_of_category(log_dir, category='PEDESTRIAN')\n",
    "peds_on_road = on_road(peds, log_dir)\n",
    "jaywalking_peds = scenario_not(at_pedestrian_crossing)(peds_on_road, log_dir)\n",
    "\n",
    "vehicles = get_objects_of_category(log_dir, category='VEHICLE')\n",
    "moving_vehicles = scenario_and([in_drivable_area(vehicles, log_dir), scenario_not(stationary)(vehicles, log_dir)])\n",
    "crossed_vehicles = being_crossed_by(moving_vehicles, jaywalking_peds, log_dir)\n",
    "behind_crossed_vehicle = get_objects_in_relative_direction(crossed_vehicles, moving_vehicles, log_dir,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdirection='backward', max_number=1, within_distance=25)\n",
    "\n",
    "#Output scenario outputs a .pkl for the predicted tracks during that scenario\n",
    "output_scenario(behind_crossed_vehicle, description, log_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to define a scenario, let's let an LLM do it for us.\n",
    "\n",
    "Using this function requires an [Anthropic API](https://www.anthropic.com/api) key!\n",
    "\n",
    "Since this can get quite expensive,\n",
    "we provide the predicted scenario definitions in the output/llm_scenario_predictions folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refAV.scenario_prediction import predict_scenario_from_description\n",
    "from refAV.paths import LLM_DEF_DIR\n",
    "\n",
    "predict_scenario_from_description('vehicle heading toward ego from the side while at an intersection', LLM_DEF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basics out of the way, let's run evaluation on the entire validation dataset.\n",
    "The create_base_prediction function calls the LLM scenario definition generator and the\n",
    " runs the defintion to find instance of the prompt.\n",
    "  It can take quite a bit of time to go through all of the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from refAV.eval import create_baseline_prediction\n",
    "\n",
    "log_prompt_input_path = Path('av2_sm_downloads/log_prompt_pairs_val.json')\n",
    "with open(log_prompt_input_path, 'rb') as f:\n",
    "\tlog_prompts = json.load(f)\n",
    "\n",
    "for log_id, prompts in tqdm(log_prompts.items()):\n",
    "\tfor prompt in prompts:\n",
    "\t\tcreate_baseline_prediction(prompt, log_id, SM_PRED_DIR, LLM_DEF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combine_matching_pkls function will combine all prediction and ground truth .pkl files into a single .pkl file. This is the .pkl file that is used for submission to the leaderboard. Running evaluate_pkls will the predicted tracks across four metrics: HOTA-Temporal, HOTA, timestamp-level F1, and scenario-level F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refAV.eval import combine_matching_pkls, evaluate_pkls\n",
    "\n",
    "eval_output_dir = Path(f'output/evaluation/val')\n",
    "combine_matching_pkls(SM_DATA_DIR, SM_PRED_DIR, eval_output_dir)\n",
    "evaluate_pkls(eval_output_dir / 'combined_predictions.pkl', eval_output_dir / 'combined_gt.pkl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
